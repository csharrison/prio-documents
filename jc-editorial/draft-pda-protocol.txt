



HTTPBIS                                                        S. People
Internet-Draft                                                 Somewhere
Intended status: Standards Track                            27 July 2021
Expires: 28 January 2022


                   Private Data Aggregation Protocol
                       draft-pda-protocol-latest

Abstract

   TODO: writeme

Discussion Venues

   This note is to be removed before publishing as an RFC.

   Discussion of this document takes place on the mailing list (), which
   is archived at .

   Source for this draft and an issue tracker can be found at
   https://github.com/abetterinternet/prio-documents.

Status of This Memo

   This Internet-Draft is submitted in full conformance with the
   provisions of BCP 78 and BCP 79.

   Internet-Drafts are working documents of the Internet Engineering
   Task Force (IETF).  Note that other groups may also distribute
   working documents as Internet-Drafts.  The list of current Internet-
   Drafts is at https://datatracker.ietf.org/drafts/current/.

   Internet-Drafts are draft documents valid for a maximum of six months
   and may be updated, replaced, or obsoleted by other documents at any
   time.  It is inappropriate to use Internet-Drafts as reference
   material or to cite them other than as "work in progress."

   This Internet-Draft will expire on 28 January 2022.

Copyright Notice

   Copyright (c) 2021 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents (https://trustee.ietf.org/
   license-info) in effect on the date of publication of this document.
   Please review these documents carefully, as they describe your rights
   and restrictions with respect to this document.  Code Components
   extracted from this document must include Simplified BSD License text
   as described in Section 4.e of the Trust Legal Provisions and are
   provided without warranty as described in the Simplified BSD License.

Table of Contents

   1.  Introduction
     1.1.  DISCLAIMER
     1.2.  Terminology
   2.  Overview
     2.1.  Private aggregation via secret sharing
     2.2.  Validating inputs in zero knowledge
     2.3.  Collecting reports
     2.4.  Data flow
   3.  PDA protocols
     3.1.  Configuration
       3.1.1.  Tasks
       3.1.2.  HPKE key configuration
     3.2.  Pre-conditions
     3.3.  Upload
       3.3.1.  Key Config Request
       3.3.2.  Upload Request
     3.4.  Collect
       3.4.1.  Collect Request
       3.4.2.  Verifying and Aggregating Reports
     3.5.  Error handling
     3.6.  Common abort conditions
   4.  Prio
     4.1.  Parameters
       4.1.1.  Finite field arithmetic
       4.1.2.  Pseudorandom number generation
   5.  Hits
   6.  System design
     6.1.  Aggregator discovery
     6.2.  Share uploading
     6.3.  Open questions and system parameters
   7.  Operational Considerations
     7.1.  Data resolution limitations
     7.2.  Aggregation utility and soft batch deadlines
     7.3.  Data integrity constraints
   8.  Security Considerations
     8.1.  Security overview
       8.1.1.  Threat model
       8.1.2.  Future work and possible extensions
       8.1.3.  Security considerations
     8.2.  System requirements
       8.2.1.  Data types
   9.  IANA Considerations
   10. References
     10.1.  Normative References
     10.2.  Informative References
   Author's Address

1.  Introduction

   This document describes a framework for specifying protocols for
   privacy-preserving data-aggregation (abbreviated PDA).  Each protocol
   is executed by a large set of clients and a small set of servers.
   The servers' goal is to compute some aggregate statistic over the
   clients' inputs without learning the inputs themselves.  This is made
   possible by distributing the computation among the servers in such a
   way that, as long as at least one of them executes the protocol
   honestly, no input is ever seen in the clear by any server.

1.1.  DISCLAIMER

   This document is a work in progress.  We have not yet settled on the
   design of the protocol framework or the set of features we intend to
   support.

1.2.  Terminology

   This section defines some terminology we will use in the remainder of
   this document.

   1.   Aggregation function: The function computed over the users'
        inputs.

   2.   Aggregator: An endpoint that runs the input-validation protocol
        and accumulates input shares.

   3.   Batch: A set of reports that are aggregated into an output.

   4.   Batch size: The minimum size of a batch.

   5.   Batch window: The minimum time difference between the oldest and
        newest report in a batch (in seconds).

   6.   Client: The endpoint from which a user sends data to be
        aggregated, e.g., a web browser.

   7.   Collector: The endpoint that receives the output of the
        aggregation function.  It also specifies the parameters of the
        protocol.

   8.   False input: An input that is valid, but incorrect.  For
        example, if the data being gathered is whether or not users have
        clicked on a particular button, a client could report clicks
        when none occurred.

   9.   Input: The measurement (or measurements) emitted by a client,
        before any encryption or secret sharing scheme is applied.

   10.  Input share: One of the shares output by feeding an input into a
        secret sharing scheme.  Each share is to be transmitted to one
        of the participating aggregators.

   11.  Input validation protocol: The protocol executed by the client
        and aggregators in order to validate the client's input without
        leaking its value to the aggregators.

   12.  Invalid input: An input for which the input validation protocol
        fails.  For example, if the input is meant to be bit vectors,
        then "[2, 1, 0]" is invalid.

   13.  Measurement: A single value (e.g., a count) being reported by a
        client.  Multiple measurements may be grouped into a single
        protocol input.

   14.  Leader: A distinguished aggregator that coordinates input
        validation and data collection.

   15.  Helper: [TODO: Define Helper in a useful way]

   16.  Output: A reduction over the inputs, for instance a statistical
        aggregation, which is of interest to a collector.  This is the
        output of the aggregation function.

   17.  Output share: The share of an output emitted by an aggregator.
        Output shares can be reassembled by the leader into the final
        output.

   18.  Prio v1: Mozilla's Origin Telemetry project
        (https://blog.mozilla.org/security/2019/06/06/next-steps-in-
        privacy-preserving-telemetry-with-prio/).

   19.  Prio v2: Contact tracing project by Apple, Google, and ISRG.

   20.  Proof: A value generated by the client used by the aggregators
        to verify the client's input.

   21.  Proof share: A share of a proof, used by an aggregator during
        the input-validation protocol.

   22.  Report: Uploaded to the leader from the client.  A report is
        encrypted, containing both input shares and proof shares.

   23.  Server: An aggregator or collector.

2.  Overview

   [OPEN ISSUE: Rework this section in light of issue #44.]

   The protocol is executed by a large set of clients and a small set of
   servers.  We call the servers the _aggregators_. Each client's input
   to the protocol is a set of measurements (e.g., counts of some user
   behavior).  Given the input set of measurements "x_1, ..., x_n" held
   by "n" users, the goal of a _private data aggregation (PDA) protocol_
   is to compute "y = F(x_1, ..., x_n)" for some aggregation function
   "F" while revealing nothing else about the measurements.

2.1.  Private aggregation via secret sharing

   The main cryptographic tool used for achieving this privacy goal is
   _additive secret sharing_. Rather than send its input in the clear,
   each client splits its measurements into a sequence of _shares_ and
   sends a share to each of the aggregators.  Additive secret sharing
   has two important properties: - It's impossible to deduce the
   measurement without knowing _all_ of the shares. - It allows the
   aggregators to compute the final output by first adding up their
   measurements shares locally, then combining the results to obtain the
   final output.

   Consider an illustrative example.  Suppose there are three clients
   and two aggregators.  Each client "i" holds a single measurement in
   the form of a positive integer "x[i]", and our goal is to compute the
   sum of the measurements of all clients.  In this case, the protocol
   input is a single measurement consisting of a single positive
   integer; no additional encoding is done.  Given this input, the first
   client splits its measurement "x[1]" with additive secret-sharing
   into a pair of integers "X[1,1]" and "X[1,2]" for which "x[1]" is
   equal to "X[1,1] + X[1,2]" modulo a prime number "p".  (For
   convenience, we will omit the mod "p" operator in the rest of this
   section.)  It then uploads "X[1,1]" to one server "X[1,2]" to the
   other.  The second client splits its measurement "x[2]" into "X[1,2]"
   and "X[2,2]", uploads them to the servers, and so on.

   Now the first aggregator is in possession of shares "X[1,1]",
   "X[2,1]", and "X[3,1]" and the second aggregator is in possession of
   shares "X[2,1]", "X[2,2]", and "X[2,3]".  Each aggregator computes
   the sum of its shares; let "A[1]" denote the first aggregator's share
   of the sum and let "A[2]" denote the second aggregator's share of the
   sum.  In the last step, aggregators combine their sum shares to
   obtain the final output "y = A[1] + A[2]".  This is correct because
   modular addition is commutative.  I.e.,

       y = A[1] + A[2]
         = (x[1,1] + x[2,1] + x[3,1]) + (x[1,2] + x[2,2] + x[3,2])
         = (x[1,1] + x[1,2]) + (x[2,1] + x[2,2]) + (x[3,1] + x[3,2])
         = x[1] + x[2] + x[3]
         = F(x[1], x[2], x[3])

   *Prio.* This approach can be used to privately compute any function
   "F" that can be expressed as a function of the sum of the users'
   inputs.  In Prio [CB17], each user splits its input into shares and
   sends each share to one of the aggregators.  The aggregators sum up
   their input shares.  Once all the shares have been aggregated, they
   combine their shares of the aggregate to get the final output.

   Not all aggregate functions can be expressed this way efficiently,
   however.  Prio supports only a limited set of aggregation functions,
   some of which we highlight below:

   *  Simple statistics, like sum, mean, min, max, variance, and
      standard deviation;

   *  Histograms with fixed bin sizes (also allows estimation of
      quantiles, e.g., the median);

   *  More advanced statistics, like linear regression;

   *  Bitwise-OR and -AND on bit strings; and

   *  Computation of data structures, like Bloom filters, counting Bloom
      filters, and count-min sketches, that approximately represent
      (multi-)sets of strings.

   This variety of aggregate types is sufficient to support a wide
   variety of data aggregation tasks.

   *Hits.* A common PDA task that can't be solved efficiently with Prio
   is the "t"-_heavy-hitters_ problem [BBCp21].  In this setting, each
   user is in possession of a single "n"-bit string, and the goal is to
   compute the compute the set of strings that occur at least "t" times.
   One reason that Prio doesn't apply to this problem is that the proof
   generated by the client would be huge.

   [TODO: Provide an overview of the protocol of [BBCp21] and provide
   some intuition about how additive secret sharing is used.]

2.2.  Validating inputs in zero knowledge

   An essential task of any data collection pipeline is ensuring that
   the input data is "valid".  Going back to the example above, it's
   often useful to assert that each measurement is in a certain range,
   e.g., "[0, 2^k)" for some "k".  This straight-forward task is
   complicated in our setting by the fact that the inputs are secret
   shared.  In particular, a malicious client can corrupt the
   computation by submitting random integers instead of a proper secret
   sharing of a valid input.

   To solve this problem, in each PDA protocol, the client generates a
   zero-knowledge proof of its input's validity that the aggregators use
   to verify that their shares correspond to as valid input.  The
   verification procedure is designed to ensure that the aggregators
   learn nothing about the input beyond its validity.

   After encoding its measurements as an input to the PDA protocol, the
   client generates a _proof_ of the input's validity.  It then splits
   the proof into shares and sends a share of both the proof and input
   to each aggregator.  The aggregators use their shares of the proof to
   decide if their input shares correspond to a valid input.

2.3.  Collecting reports

   As noted above, each client has a collection of measurements that it
   wants to send.  Each measurement is characterized by a set of
   parameters that are centrally configured and provided to each client:

   *  A unique identifier (e.g., "dns-queries-mean")

   *  A description of how to collect the measurement (e.g., "count the
      number of DNS queries")

   *  The statistic to be computed over the measurement values (e.g.,
      mean)

   *  The rules for what constitutes a valid value (e.g., must be
      between 0 and 10000)

   Once the client has collected the measurements to send, it needs to
   turn them into a set of reports.  Naively, each measurement would be
   sent in its own report, but it is also possible to have multiple
   measurements in a single report; clients need to be configured with
   the mapping from measurements to reports.  The set of measurements
   that go into a report is referred to as the "input" to the report.
   Because each report is independent, for the remainder of this
   document we focus on a single report and its inputs.

   [NOTE(cjpatton): This paragraph is slightly misleading.  If you want
   to do a range check for the measurement (this will usually be
   necessary, IMO) then you'll need a few extra field elements to encode
   the input.]  The client uses the statistic to be computed in order to
   know how to encode the measurement.  For instance, if the statistic
   is mean, then the measurement can be encoded directly.  However, if
   the statistic is standard deviation, then the client must send both
   "x" and "x^2".  Section [TODO: cite to internal description of how to
   encode] describes how to encode measurements for each statistic.  The
   client uses the validity rules to construct the zero knowledge proof
   showing that the encoded measurement is valid.

2.4.  Data flow

   Each PDA task consists of two sub-protocols, _upload_ and _collect_,
   which are executed concurrently.  Each sub-protocol consists of a
   sequence of HTTP requests made from one entity to another.

   +-------------+ 1.      +-------------+
   |             +--------->             |
   |   Client    |         |   Leader    |
   |             |    +----+             |
   +------+------+    | 2. +------^------+
          | 1.        |           |
          |           |           |
          |           |           | 2.
   +------v------+    |    +------+------+
   |             <----+    |             |
   |   Helper    |         |  Collector  |
   |             |         |             |
   +-------------+         +-------------+

      Figure 1: Who makes requests to whom while executing a PDA task.

   1.  *Upload:* Each client assembles its measurements into an input
       for the given PDA protocol.  It generates a proof of its input's
       validity and splits the input and proof into two shares, one for
       the leader and another for a helper.  The client then encrypts
       the leader's share and helper's share under, respectively, the
       leader's public key and the helper's public key.  (The keys are
       obtained by making requests to the leader and helper.)  Finally,
       the client uploads the encrypted shares to the leader.

   2.  *Collect:* The collector makes one or more requests to the leader
       in order to obtain the final output of the protocol.  Before the
       output can be computed, the aggregators (i.e, the leader and
       helper) need to have verified and aggregated a sufficient number
       of inputs.  Depending on the PDA protocol, it may be possible for
       the aggregators to do so immediately when reports are uploaded.
       (See Section 4.)  However, in general it is necessary for them to
       wait until (a) enough reports have been uploaded and (b) the
       collector has made a request.  (See Section 5.)

3.  PDA protocols

   This section specifies a protocol for executing generic PDA tasks.
   Concrete PDA protocols are described in Section 4 and Section 5.

   Each round of the protocol corresponds to an HTTP request and
   response.  The content type of each request is "application/octet-
   stream".  We assume that some transport layer security protocol
   (e.g., TLS or QUIC) is used between each pair of parties and that the
   server is authenticated.

   [TODO: Decide how to provide mutual authentication in leader-to-
   helper and collector-to-leader connections.  One option is to use
   client certificates for TLS; another is to have the leader sign its
   messages directly, as in Prio v2.  For collector-to-leader
   connections, we may just have this be up to deployment.  (For
   instance, the collector might authenticate themselves by logging into
   a website that has some trust relationship with the leader.)]

   [TODO: @chris-wood suggested we specify APIs for producing and
   consuming each of the messages in the protocol.  Specific PDA
   protocols would implement this API.]

   *Error handling.* In this section, we will use the verbs "abort" and
   "alert with "[some error message]"" to describe how protocol
   participants react to various error conditions.  The behavior is
   specified in Section 3.5.  For common errors, we may elide the verbs
   altogether and refer to Section 3.6.

   [TODO: Fix the bounds for length-prefixed parameters in protocol
   messages.  (E.g., "<23..479>" instead of "<1..2^16-1>".)]

3.1.  Configuration

3.1.1.  Tasks

   Each PDA protocol is associated with a _PDA task_ that specifies the
   measurements that are to be collected.  Associated to each task is a
   set of _PDA Parameters_, encoded by the following "PDAParam"
   structure, which specify the protocol used to verify and aggregate
   the clients' measurements:

   struct {
     opaque nonce[16];
     Url leader_url;
     Url helper_url;
     HpkeConfig collector_config; // [TODO: Remove this?]
     uint64 batch_size;
     Duration batch_window;
     PDAProto proto;
     uint16 length; // Length of the remainder.
     select (PDAClientParam.proto) {
       case prio: PrioParam;
       case hits: HitsParam;
     }
   } PDAParam;

   enum { prio(0), hits(1) } PDAProto;

   opaque Url<1..2^16-1>;

   Duration uint64; /* Number of seconds elapsed between two instants */

   Time uint64; /* seconds elapsed since start of UNIX epoch */

   *  "nonce": A unique sequence of bytes used to ensure that two
      otherwise identical "PDAParam" instances will have distinct
      "PDATaskID"s.  It is RECOMMENDED that this be set to a random
      16-byte string derived from a cryptographically secure
      psuedorandom number generator.

   *  "leader_url": The leader's endpoint URL.

   *  "helper_url": The helper's endpoint URL.

   *  "collector_config": The HPKE configuration of the collector
      (described in Section 3.1.2).  [OPEN ISSUE: Maybe the collector's
      HPKE config should be carried by the collect request?]

   *  "batch_size": The batch size, i.e., the minimum number of reports
      that are aggregated into an output.

   *  "batch_window": The batch window, i.e., the minimum time
      difference between the oldest and newest report in a batch.

   *  "proto": The PDA protocol, e.g., Prio or Hits.  The rest of the
      structure contains the protocol specific parameters.

   Each task has a unique _task ID_ derived from the PDA parameters:

   opaque PDATaskID[32];

   The task ID of a "PDAParam" `is derived using the following
   procedure:

   task_id = SHA-256(param)

   Where "SHA-256" is as specified in [FIPS180-4].

3.1.2.  HPKE key configuration

   Our protocol uses HPKE for public-key encryption
   [I-D.irtf-cfrg-hpke].  Each aggregator specifies the HPKE public key
   that clients use to encrypt its input share, and the collector
   specifies the HPKE public key that helpers use to encrypt output
   shares during collection.  The public key and associated parameters
   are structured as follows:

   struct {
     uint8 id;
     HpkeKemId kem_id;
     HpkeKdfId kdf_id;
     HpkeAeadKdfId aead_id;
     HpkePublicKey public_key;
   } HpkeConfig;

   opaque HpkePublicKey<1..2^16-1>;
   uint16 HpkeAeadId; // Defined in I-D.irtf-cfrg-hpke
   uint16 HpkeKemId;  // Defined in I-D.irtf-cfrg-hpke
   uint16 HpkeKdfId;  // Defined in I-D.irtf-cfrg-hpke

   [TODO: Decide whether to use the same config structure as OHTTP/ECH.
   This would add support for multiple cipher suites.]

   We call this a _key configuration_. The key configuration is used to
   set up a base-mode HPKE context to use to derive symmetric keys for
   protecting: (1) input shares sent from the client to an aggregator;
   or (2) output shares sent from the helper to the collector.  The
   _config id_, "HpkeConfig.id", is forwarded by the sender to the
   receiver to help the receiver decide if it knows the decryption key.

3.2.  Pre-conditions

   We assume the following conditions hold before execution of any PDA
   task begins:

   1.  The aggregators agree on a set of PDA tasks, as well as the PDA
       parameters associated to each task.

   2.  Each aggregator has a clock that is roughly in sync with true
       time, i.e., within the batch window specified by the PDA
       parameters.  (This is necessary to prevent the same report from
       appearing in multiple batches.)

   3.  Each client has selected a PDA task for which it will upload a
       report.  It is also configured with the task's parameters.

   4.  Each client and the leader can establish a leader-authenticated
       secure channel.

   5.  The leader and each helper can establish a helper-authenticated
       secure channel.

   6.  The collector and leader can establish a leader-authenticated
       secure channel.

   7.  The collector has chosen an HPKE configuration and corresponding
       secret key.

   8.  Each aggregator has chosen an HPKE configuration and
       corresponding secret key.

   [TODO: It would be clearer to include a "pre-conditions" section
   prior to each "phase" of the protocol.]

3.3.  Upload

   Client          Leader         Helper
     |  key config  |              |
     <-------------->              |
     |              |  key config  |
     <----------------------------->
     |  upload      |              |
     <-------------->              |
     v              v              v

                    Figure 2: Flow of the upload process

   [NOTE: @acmiyaguchi pointed out that the use of an anonymizing proxy
   for uploading shares might be easier to implement if the "upload"
   phase involved a single HTTP request.  However, OHTTP
   (https//www.ietf.org/archive/id/draft-thomson-http-oblivious-01.html)
   allows clients to make multiple requests through a proxy, so these
   kinds of use cases should work.]

3.3.1.  Key Config Request

   Before the client can upload its report to the leader, it must first
   discover the key configs of each of the aggregators.  To do so, the
   client sends a GET request to "[aggregator]/key_config", where
   "[aggregator]" is the aggregator's endpoint URL.  The aggregator
   responds to well-formed requests with status 200 and an "HpkeConfig".

   The client issues a key config request to "PDAParam.leader_url" and
   "PDAParam.helper_Url".  It aborts if any of the following happen for
   either request:

   *  the client and aggregator failed to establish a secure,
      aggregator-authenticated channel;

   *  the GET request failed or didn't return a valid key config; or

   *  the key config specifies a KEM, KDF, or AEAD algorithm the client
      doesn't recognize.

   [OPEN ISSUE: @chris-wood: Can't the leader determine if helpers are
   "online"?  This seems to reveal information that's specific to
   clients.  Imagine, for example, that clients are prohibited from
   talking to helpers but not the leader.  Is it OK that leaders learn
   that about a client?  I'm not sure, so I'd be inclined to remove this
   unless we have a concrete use case.]

3.3.2.  Upload Request

   Next, the client issues a POST request to "[leader]/upload", where
   "[leader]" is the leader's endpoint URL.  The payload is structured
   as follows:

   struct {
     PDATaskID task_id;
     Time time;
     PDAEncryptedInputShare encrypted_input_shares<1..2^16-1>;
   } PDAUploadReq;

   We sometimes refer to this message as the _report_. The message
   contains the "task_id" of the previous request.  It also includes the
   time at which the report was generated.  This field is present to
   ensure that each report is included in at most one batch.  The rest
   of the message consists of the encrypted input shares, each of which
   has the following structure:

   struct {
     uint8 config_id;
     opaque enc<1..2^16-1>;
     opaque payload<1..2^16-1>;
   } PDAEncryptedInputShare;

   *  "config_id" is equal to "HpkeConfig.id", where "HpkeConfig" is the
      aggregator's key config.

   *  "enc" is the encapsulated HPKE context, used by the aggregator to
      decrypt its input share.

   *  "payload" is the encrypted input share.

   To generate the report, the client begins by encoding its
   measurements as an input for the PDA protocol and splitting it into
   input shares.  (Note that the structure of each input share depends
   on the PDA protocol in use, its parameters, and the role of
   aggregator, i.e., whether the aggregator is a leader or helper.)  To
   encrypt an input share, the client first generates an HPKE
   [I-D.irtf-cfrg-hpke] context for the aggregator by running

   enc, context = SetupBaseS(pk, [TODO])

   where "pk" is the aggregator's public key. "enc" is the encapsulate
   HPKE context and "context" is the HPKE context used by the client for
   encryption.  The payload is encrypted as

   payload = context.Seal(input_share, [TODO])

   where "input_share" is the aggregator's input share.

   [TODO: Fully specify encryption of the shares.  We need to make sure
   we authenticate things like the PDA parameters and the report
   timestamp.  We need to decide what the info string for SetupBaseS()
   will be, as well as the aad for context.Seal().  The aad might be the
   entire "transcript" between the client and aggregator.]

   The leader responds to well-formed requests to "[leader]/upload" with
   status 200 and an empty body.  Malformed requests are handled as
   described in Section 3.6.

3.4.  Collect

   Collector     Leader           Helper
     |  collect 1  |                |
     +------------->                |
     |             |  aggregate 1   |
     |             <---------------->
     |             |  ...           |
     |             |  aggregate L   |
     |             <---------------->
     |             |  output share  |
     |             <---------------->
     <-------------+                |
     |  ...        |                |
     |  collect N  |                |
     +------------->                |
     |             |  aggregate 1   |
     |             <---------------->
     |             |     ...        |
     |             |  aggregate L   |
     |             <---------------->
     |             |  output share  |
     |             <---------------->
     <-------------+                |
     v             v                v

     Figure 3: Flow of the collect process with N collect requests and
                 L aggregate requests per collect request.

   [TODO: Decide if and how the collector's request is authenticated.]

   The collector interacts with the leader to produce the final
   aggregate output.  This process consists of a sequence of _collect
   requests_ issued to the leader.  Before a request can succeed, the
   aggregators must have verified and aggregated enough reports and the
   leader must have obtained the helper's encrypted output share (see
   Section 3.4.2).  In general, the procedure by which the aggregators
   verify and aggregate reports depends on parameters carried by the
   collect request.  This procedure is described below in Section 3.4.2.

3.4.1.  Collect Request

   [TODO: Decide whether to specify things in terms of functions. @ekr
   pointed out that it would be clearer to just talk about protocol
   messages.]

   The collect protocol is an iterative procedure driven by the
   collector and parameterized by a PDAParam.  At a high level, it
   proceeds as follows.  First, the collector initializes local per-
   protocol state using the corresponding PDAParam.  This state object
   has the following member functions:

   *  CreateRequest(): Creates a PDACollectReq object, defined below,
      that is sent to the leader to complete one iteration of the
      protocol.  Along with any protocol-specific parameters, the
      message specifies a time interval "[batch_start, batch_end)" that
      determines the batch of reports to be aggregated.  It must be that
      "batch_end - batch_start >= PDAParam.batch_window" and
      "batch_start" and "batch_end" are multiples of
      "PDAParam.batch_window".

   struct {
     PDATaskID task_id;
     Time batch_start; // The beginning of the batch.
     Time batch_end;   // The end of the batch (exclusive).
     PDAProto proto;
     select (PDACollectReq.proto) {
       case prio: PrioCollectReq;
       case hits: HitsCollectReq;
     }
   } PDACollectReq;

   *  Update(resp: PDACollectResp): Consumes an opaque PDACollectResp
      object, defined below, that is received from the leader in
      response to a PDACollectReq.

   struct {
     PDATaskID task_id;
     PDAProto proto;
     PDAOutputShare leader_share;
     opaque encrypted_helper_share;
   } PDACollectResp;

   *  Finished(): Returns a boolean indicating indicating whether or not
      the collection procedure is complete.  This function returns true
      when the collect iteration is complete.

   *  Output(): Returns the aggregate output corresponding to the
      protocol.

   The collect procedure for a given PDAParam structure "param" is then
   driven with the following algorithm:

   state = CreateState(param)
   while not state.Finished():
      req = state.CreateRequest()
      resp = fetch([leader]/collect, req)
      state.Update(resp)
   return state.Output()

   [OPEN ISSUE: Describe how intra-protocol errors yield collect errors
   (see issue#57).  For example, how does a leader respond to a collect
   request if the helper drops out?]

   Each collect request involves one of the helpers specified by the PDA
   parameters.  If more than one helper is specified, the collector may
   issue the requests in any order.

3.4.2.  Verifying and Aggregating Reports

   After the client uploads a report to the leader, the leader and
   helper verify in zero knowledge that the input is valid.  The exact
   procedure for doing so is protocol specific, but all protocols have
   the same basic structure.  In particular, the protocol is comprised
   of a sequence of _aggregate requests_ from the leader to the helper.
   At the end of this procedure, the leader and helper will have have
   aggregated a set of valid client inputs (though not necessarily a
   complete batch).

3.4.2.1.  Aggregate Request

   The process begins with a PDACollectReq.  The leader collects a
   sequence of reports that are all associated with the same PDA task.
   Let "[helper]" denote "PDAParam.helper_url", where "PDAParam" is the
   PDA parameters structure associated "PDAAggregateReq.task.id".  The
   leader sends a POST request to "[helper]/aggregate" with the
   following message:

   struct {
     PDATaskID task_id;
     opaque helper_state<0..2^16>;
     PDAAggregateSubReq seq<1..2^24-1>;
   } PDAAggregateReq;

   The structure contains the PDA task, the helper's HPKE config id, an
   opaque _helper state_ string, and a sequence of _sub-requests_, each
   corresponding to a unique client report.  Sub-requests are structured
   as follows:

   struct {
     Time time; // Equal to PDAReport.time.
     PDAEncryptedInputShare helper_share;
     select (PDAParam.proto) { // PDAParam for the PDA task
       case prio: PrioAggregateSubReq;
       case hits: HitsAggregateSubReq;
     }
   } PDAAggregateSubReq;

   The "helper_share" field is the helper's encrypted input share as it
   appeared in the report uploaded by the client.  [OPEN ISSUE: We
   usually only need to send this in the first aggregate request.  Shall
   we exclude it in subsequent requests somehow?]  The "time" field
   should match the timestamp of the corresponding report.  The
   remainder of the structure is dedicated to the protocol-specific
   helper share and request parameters used for the current round.

   The helper handles well-formed requests as follows.  (As usual,
   malformed requests are handled as described in Section 3.6.)  It
   first looks for the PDA parameters "PDAParam" for which
   "PDAAggregateReq.task_id" is equal to the task ID derived from
   "PDAParam".

   The response consists of the helper's updated state and a sequence of
   _sub-responses_, where the i-th sub-response corresponds to the sub-
   request for each i.  The structure of each sub-response is specific
   to the PDA protocol:

   struct {
     opaque helper_state<0..2^16>;
     PDAAggregateSubResp seq<1..2^24-1>;
   } PDAAggregateResp;

   struct {
     select (PDAParam.proto) { // PDAParam for the PDA task
       case prio: PrioAggregateSubResp;
       case hits: HitsAggregateSubResp;
     }
   } PDAAggregateSubResp;

   The helper handles each sub-request "PDAAggregateSubReq" as follows.
   It first looks up the HPKE config and corresponding secret key
   associated with "helper_share.config_id".  If not found, then the
   sub-response consists of an "unrecognized config" alert.  [TODO:
   We'll want to be more precise about what this means.  See issue#57.]
   Next, it attempts to decrypt the payload with the following
   procedure:

   context = SetupBaseR(helper_share.enc, sk, [TODO])
   input_share = context.Open(helper_share.config_id, [TODO])

   where "sk" is the HPKE secret key.  If decryption fails, then the
   sub-response consists of a "decryption error" alert.  [See issue#57.]
   Otherwise, the helper handles the request for its plaintext input
   share "input_share" and updates its state as specified by the PDA
   protocol.

   After processing all of the sub-requests, the helper encrypts its
   updated state and constructs its response to the aggregate request.

3.4.2.1.1.  Helper State

   The helper state is an optional parameter of an aggregate request
   that the can helper use to carry state across requests.  At least
   part of the state will usually need to be encrypted in order to
   protect user privacy.  However, the details of precisely how the
   state is encrypted and the information that it carries is up to the
   helper implementation.

3.4.2.2.  Output Share Request

   Once the aggregators have verified at least as many reports as
   required for the PDA task, the leader issues an _output share
   request_ to the helper.  The helper responds to this request by
   extracting its output share from its state and encrypting it under
   the collector's HPKE public key.

   The leader sends a POST request to "[helper]/output_share" with the
   following message:

   struct {
     PDATaskID task_id;
     Time batch_start; // Same as PDACollectReq.batch_start.
     Time batch_end;   // Same as PDACollectReq.batch_end.
     opaque helper_state<0..2^16>;
   } PDAOutputShareReq;

   To respond to valid output share requests, the helper first checks
   that "batch_start" and "batch_end" are multiples of
   "task.batch_window" and that "batch_end - batch_start >=
   task.batch_window".  Next, it extracts from its state the set of
   input shares that fall in the window "[batch_start, batch_end)".  If
   the size of the batch is less than "task.batch_size", then it aborts
   and alerts the leader with "insufficient data".  Otherwise, it
   computes its output share, which has the following structure:

   struct {
     PDAProto proto;
     select (PDAOutputShare.proto) {
       case prio: PrioOutputShare;
       case hits: HitsOutputShare;
     }
   } PDAOutputShare;

   Next, it encrypts its output share under the collector's HPKE public
   key:

   enc, context = SetupBaseS(pk, [TODO])
   encrypted_output_share = context.Seal(output_share, [TODO])

   where "pk" is the HPKE public key encoded by the collector's HPKE key
   configuration and "output_share" is its serialized output share.

   It responds with the following message:

   struct {
     uint8 collector_hpke_config_id;
     opaque enc<1..2^16-1>;
     opaque encrypted_output_share<1..2^16>;
   } PDAOutputShareResp;

   The leader uses the helper's output share response to respond to the
   collector's collect request (see Section 3.4).

3.5.  Error handling

   An _alert_ is a message sent either in an HTTP request or response
   that signals to the receiver that the peer has aborted the protocol.
   The payload is

   struct {
     PDATaskID task_id;
     opaque payload<1..255>;
   } PDAAlert;

   where "task" is the associated PDA task (this value is always known)
   and "payload" is the message.  When sent by an aggregator in response
   to an HTTP request, the response status is 400.  When sent in a
   request to an aggregator, the URL is always "[aggregator]/error",
   where "[aggregator]" is the URL of the aggregator endpoint.

3.6.  Common abort conditions

   The following specify the "boiler-plate" behavior for various error
   conditions.

   *  The message type for the payload of each request and response is
      unique for a given URL.  If ever a client, aggregator, or
      collector receives a request or response to a request with a
      malformed payload, then the receiver aborts and alerts the peer
      with "unrecognized message".

   *  Each POST request to an aggregator contains a "PDATaskID".  If the
      aggregator does not recognize the task, i.e., it can't find a
      "PDAParam" for which the derived task ID matches the "PDATaskID",
      then it aborts and alerts the peer with "unrecognized task".

4.  Prio

   [TODO: Define Prio-specific protocol messages.]

4.1.  Parameters

4.1.1.  Finite field arithmetic

   The algorithms that comprise the input-validation protocol --- Prove,
   Query, and Decide --- are constructed by generating and evaluating
   polynomials over a finite field.  As such, the main ingredient of
   Prio is an implementation of arithmetic in a finite field suitable
   for the given application.

   We will use a prime field.  The choice of prime is influenced by the
   following criteria:

   1.  *Field size.* How big the field needs to be depends on the type
       of data being aggregated and how many users there are.  The field
       size also impacts the security level: the longer the validity
       circuit, the larger the field needs to be in order to effectively
       detect malicious clients.  Typically the soundness error (i.e.,
       the probability of an invalid input being deemed valid by the
       aggregators) will be 2n/(p-n), where n is the size of the input
       and p is the prime modulus.

   2.  *Fast polynomial operations.* In order to make Prio practical,
       it's important that implementations employ FFT to speed up
       polynomial operations.  In particular, the prime modulus p should
       be chosen so that (p-1) = 2^b * s for large b and odd s.  Then
       g^s is a principle, 2^b-th root of unity (i.e., g^(s*2^b) = 1),
       where g is the generator of the multiplicative subgroup.  This
       fact allows us to quickly evaluate and interpolate polynomials at
       2^a-th roots of unity for any 1 <= a <= b.  Note that b imposes
       an upper bound on the size of proofs, so it should be large
       enough to accommodate all foreseeable use cases.  Something like
       b >= 20 is probably good enough.

   3.  *As close to a power of two as possible.* We use rejection
       sampling to map a PRNG seed to a pseudorandom sequence of field
       elements (see Section 4.1.2).  In order to minimize the
       probability of a simple being rejected, the modulus should be as
       close to a power of 2 as possible.

   4.  *Code optimization.* [[TODO: What properties of the field make it
       possible to write faster implementations?]]

   The table below lists parameters that meet these criteria at various
   levels of security.  (Note that #1 is the field used in "Prio v2".)
   The "size" column indicates the number of bits required to represent
   elements of the field.

   +==+======+========================================+====+=====+=====+
   |# | size | p                                      | g  | b   |s    |
   +==+======+========================================+====+=====+=====+
   |1 | 32   | 4293918721                             | 19 | 20  |3^2 *|
   |  |      |                                        |    |     |5 * 7|
   |  |      |                                        |    |     |* 13 |
   +--+------+----------------------------------------+----+-----+-----+
   |2 | 64   | 15564440312192434177                   | 5  | 59  |3^3  |
   +--+------+----------------------------------------+----+-----+-----+
   |3 | 80   | 779190469673491460259841               | 14 | 72  |3 * 5|
   |  |      |                                        |    |     |* 11 |
   +--+------+----------------------------------------+----+-----+-----+
   |4 | 123  | 9304595970494411110326649421962412033  | 3  | 120 |7    |
   +--+------+----------------------------------------+----+-----+-----+
   |5 | 126  | 74769074762901517850839147140769382401 | 7  | 118 |3^2 *|
   |  |      |                                        |    |     |5^2  |
   +--+------+----------------------------------------+----+-----+-----+

                                  Table 1

   [TODO: Choose new parameters for 2, 3, and 5 so that p is as close to
   2^size as possible without going over. (4 is already close enough; 1
   is already deployed and can't be changed.]

   *Finding suitable primes.* One way to find suitable primes is to
   first choose choose b, then "probe" to find a prime of the desired
   size.  The following SageMath script prints the parameters of a
   number of (probable) primes larger than 2^b for a given b:

  b = 116
  for s in range(0,1000,1):
      B = 2^b
      p = (B*s).next_prime()
      if p-(B*s) == 1:
          bits = round(math.log2(p), 2)
          print(bits, p, GF(p).multiplicative_generator(), b, factor(s))

4.1.2.  Pseudorandom number generation

   A suitable PRNG will have the following syntax.  Fix a finite field
   K:

   1.  x := PRNG(k, n) denotes generation of a vector of n elements of
       K.

   This can be instantiated using a standard stream cipher, e.g., AES-
   CTR, as follows.  Interpret the seed k as the key and IV for
   generating the AES-CTR key stream.  Proceed by rejection sampling, as
   follows.  Let m be the number of bits needed to encode an element of
   K.  Generate the next m bits of key stream and interpret the bytes as
   an integer x, clearing the most significant m - l bits, where l is
   the bit-length of the modulus p.  If x < p, then output x.
   Otherwise, generate the next m bits of key stream and try again.
   Repeat this process indefinitely until a suitable output is found.

5.  Hits

   [TODO: Define Hits-specific protocol messages.]

6.  System design

   [[OPEN ISSUE: This section seems like a catch-all for things not in
   other sections.  Perhaps there is a natural home for aggregator
   discovery, share uploading, open issues, and system parameters?]]

6.1.  Aggregator discovery

   [[OPEN ISSUE: writeme]]

6.2.  Share uploading

   [[OPEN ISSUE: writeme]]

6.3.  Open questions and system parameters

   [[OPEN ISSUE: discuss batch size parameter and thresholds]] [[OPEN
   ISSUE: discuss f^ leakage differences from [GB17]]]

7.  Operational Considerations

   Prio has inherent constraints derived from the tradeoff between
   privacy guarantees and computational complexity.  These tradeoffs
   influence how applications may choose to utilize services
   implementing the specification.

7.1.  Data resolution limitations

   Privacy comes at the cost of computational complexity.  While affine-
   aggregatable encodings (AFEs) can compute many useful statistics,
   they require more bandwidth and CPU cycles to account for finite-
   field arithmetic during input-validation.  The increased work from
   verifying inputs decreases the throughput of the system or the inputs
   processed per unit time.  Throughput is related to the verification
   circuit's complexity and the available compute-time to each
   aggregator.

   Applications that utilize proofs with a large number of
   multiplication gates or a high frequency of inputs may need to limit
   inputs into the system to meet bandwidth or compute constraints.
   Some methods of overcoming these limitations include choosing a
   better representation for the data or introducing sampling into the
   data collection methodology.

   [[TODO: Discuss explicit key performance indicators, here or
   elsewhere.]]

7.2.  Aggregation utility and soft batch deadlines

   A soft real-time system should produce a response within a deadline
   to be useful.  This constraint may be relevant when the value of an
   aggregate decreases over time.  A missed deadline can reduce an
   aggregate's utility but not necessarily cause failure in the system.

   An example of a soft real-time constraint is the expectation that
   input data can be verified and aggregated in a period equal to data
   collection, given some computational budget.  Meeting these deadlines
   will require efficient implementations of the input-validation
   protocol.  Applications might batch requests or utilize more
   efficient serialization to improve throughput.

   Some applications may be constrained by the time that it takes to
   reach a privacy threshold defined by a minimum number of input
   shares.  One possible solution is to increase the reporting period so
   more samples can be collected, balanced against the urgency of
   responding to a soft deadline.

7.3.  Data integrity constraints

   Data integrity concerns the accuracy and correctness of the outputs
   in the system.  The integrity of the output can be influenced by an
   incomplete round of aggregation caused by network partitions, or by
   bad actors attempting to cause inaccuracies in the aggregates.  An
   example data integrity constraint is that every share must be
   processed exactly once by all aggregators.  Data integrity
   constraints may be at odds with the threat model if meeting the
   constraints requires replaying data.

   Aggregator operators should expect to encounter invalid inputs during
   regular operation due to misconfigured or malicious clients.  Low
   volumes of errors are tolerable; the input-verification protocol and
   AFEs are robust in the face of malformed data.  Aggregators may need
   to detect and mitigate statistically significant floods of invalid or
   identical inputs that affect accuracy, e.g., denial of service (DoS)
   events.

   Certain classes of errors do not exist in the input-validation
   protocol considered in this document.  For example, packet loss
   errors when clients make requests directly to aggregators are not
   relevant when the leader proxies requests and controls the schedule
   for signaling aggregation rounds.

8.  Security Considerations

8.1.  Security overview

   Prio assumes a powerful adversary with the ability to compromise an
   unbounded number of clients.  In doing so, the adversary can provide
   malicious (yet truthful) inputs to the aggregation function.  Prio
   also assumes that all but one server operates honestly, where a
   dishonest server does not execute the protocol faithfully as
   specified.  The system also assumes that servers communicate over
   secure and mutually authenticated channels.  In practice, this can be
   done by TLS or some other form of application-layer authentication.

   In the presence of this adversary, Prio provides two important
   properties for computing an aggergation function F:

   1.  Privacy.  The aggregators and collector learn only the output of
       F computed over all client inputs, and nothing else.

   2.  Robustness.  As long as the aggregators execute the input-
       validation protocol correctly, a malicious client can skew the
       output of F only by reporting false (untruthful) input.  The
       output cannot be influenced in any other way.

   There are several additional constraints that a Prio deployment must
   satisfy in order to achieve these goals:

   1.  Minimum batch size.  The aggregation batch size has an obvious
       impact on privacy.  (A batch size of one hides nothing of the
       input.)  Section 6.3 discusses appropriate batch sizes and how
       they pertains to privacy in more detail.

   2.  Aggregation function choice.  Some aggregation functions leak
       slightly more than the function output itself.  Section 6.3
       discusses the leakage profiles of various aggregation functions
       in more detail.

8.1.1.  Threat model

   In this section, we enumerate the actors participating in the Prio
   system and enumerate their assets (secrets that are either inherently
   valuable or which confer some capability that enables further attack
   on the system), the capabilities that a malicious or compromised
   actor has, and potential mitigations for attacks enabled by those
   capabilities.

   This model assumes that all participants have previously agreed upon
   and exchanged all shared parameters over some unspecified secure
   channel.

8.1.1.1.  Client/user

8.1.1.1.1.  Assets

   1.  Unshared inputs.  Clients are the only actor that can ever see
       the original inputs.

   2.  Unencrypted input shares.

8.1.1.1.2.  Capabilities

   1.  Individual users can reveal their own input and compromise their
       own privacy.

       *  Since this does not affect the privacy of others in the
          system, it is outside the threat model.

   2.  Clients (that is, software which might be used by many users of
       the system) can defeat privacy by leaking input outside of the
       Prio system.

       *  In the current threat model, other participants have no
          insight into what clients do besides uploading input shares.
          Accordingly, such attacks are outside of the threat model.

   3.  Clients may affect the quality of aggregations by reporting false
       input.

       *  Prio can only prove that submitted input is valid, not that it
          is true.  False input can be mitigated orthogonally to the
          Prio protocol (e.g., by requiring that aggregations include a
          minimum number of contributions) and so these attacks are
          considered to be outside of the threat model.

   4.  Clients can send invalid encodings of input.

8.1.1.1.3.  Mitigations

   1.  The input validation protocol executed by the aggregators
       prevents either individual clients or coalitions of clients from
       compromising the robustness property.

8.1.1.2.  Aggregator

8.1.1.2.1.  Assets

   1.  Unencrypted input shares.

   2.  Input share decryption keys.

   3.  Client identifying information.

   4.  Output shares.

   5.  Aggregator identity.

8.1.1.2.2.  Capabilities

   1.  Aggregators may defeat the robustness of the system by emitting
       bogus output shares.

   2.  If clients reveal identifying information to aggregators (such as
       a trusted identity during client authentication), aggregators can
       learn which clients are contributing input.

       1.  Aggregators may reveal that a particular client contributed
           input.

       2.  Aggregators may attack robustness by selectively omitting
           inputs from certain clients.

           *  For example, omitting submissions from a particular
              geographic region to falsely suggest that a particular
              localization is not being used.

   3.  Individual aggregators may compromise availability of the system
       by refusing to emit output shares.

   4.  Input validity proof forging.  Any aggregator can collude with a
       malicious client to craft a proof share that will fool honest
       aggregators into accepting invalid input.

8.1.1.2.3.  Mitigations

   1.  The linear secret sharing scheme employed by the client ensures
       that privacy is preserved as long as at least one aggregator does
       not reveal its input shares.

   2.  If computed over a sufficient number of input shares, output
       shares reveal nothing about either the inputs or the
       participating clients.

8.1.1.3.  Leader

   The leader is also an aggregator, and so all the assets, capabilities
   and mitigations available to aggregators also apply to the leader.

8.1.1.3.1.  Capabilities

   1.  Input validity proof verification.  The leader can forge proofs
       and collude with a malicious client to trick aggregators into
       aggregating invalid inputs.

       *  This capability is no stronger than any aggregator's ability
          to forge validity proof shares in collusion with a malicious
          client.

   2.  Relaying messages between aggregators.  The leader can compromise
       availability by dropping messages.

       *  This capability is no stronger than any aggregator's ability
          to refuse to emit output shares.

   3.  Shrinking the anonymity set.  The leader instructs aggregators to
       construct output parts and so could request aggregations over few
       inputs.

8.1.1.3.2.  Mitigations

   1.  Aggregators enforce agreed upon minimum aggregation thresholds to
       prevent deanonymizing.

8.1.1.4.  Collector

8.1.1.4.1.  Capabilities

   1.  Advertising shared configuration parameters (e.g., minimum
       thresholds for aggregations, joint randomness, arithmetic
       circuits).

   2.  Collectors may trivially defeat availability by discarding output
       shares submitted by aggregators.

8.1.1.4.2.  Mitigations

   1.  Aggregators should refuse shared parameters that are trivially
       insecure (i.e., aggregation threshold of 1 contribution).

8.1.1.5.  Aggregator collusion

   If all aggregators collude (e.g. by promiscuously sharing unencrypted
   input shares), then none of the properties of the system hold.
   Accordingly, such scenarios are outside of the threat model.

8.1.1.6.  Attacker on the network

   We assume the existence of attackers on the network links between
   participants.

8.1.1.6.1.  Capabilities

   1.  Observation of network traffic.  Attackers may observe messages
       exchanged between participants at the IP layer.

       1.  The time of transmission of input shares by clients could
           reveal information about user activity.

           *  For example, if a user opts into a new feature, and the
              client immediately reports this to aggregators, then just
              by observing network traffic, the attacker can infer what
              the user did.

       2.  Observation of message size could allow the attacker to learn
           how much input is being submitted by a client.

           *  For example, if the attacker observes an encrypted message
              of some size, they can infer the size of the plaintext,
              plus or minus the cipher block size.  From this they may
              be able to infer which aggregations the user has opted
              into or out of.

   2.  Tampering with network traffic.  Attackers may drop messages or
       inject new messages into communications between participants.

8.1.1.6.2.  Mitigations

   1.  All messages exchanged between participants in the system should
       be encrypted.

   2.  All messages exchanged between aggregators, the collector and the
       leader should be mutually authenticated so that network attackers
       cannot impersonate participants.

   3.  Clients should be required to submit inputs at regular intervals
       so that the timing of individual messages does not reveal
       anything.

   4.  Clients should submit dummy inputs even for aggregations the user
       has not opted into.

   [[OPEN ISSUE: The threat model for Prio --- as it's described in the
   original paper and [BBG+19] --- considers *either* a malicious client
   (attacking soundness) *or* a malicious subset of aggregators
   (attacking privacy).  In particular, soundness isn't guaranteed if
   any one of the aggregators is malicious; in theory it may be possible
   for a malicious client and aggregator to collude and break soundness.
   Is this a contingency we need to address?  There are techniques in
   [BBG+19] that account for this; we need to figure out if they're
   practical.]]

8.1.2.  Future work and possible extensions

   In this section we discuss attacks that are not considered in the
   above threat model, and suggest mitigations that could be
   incorporated into implementations of this protocol or future
   revisions of this specfication.

8.1.2.1.  Client authentication

   Attackers can impersonate Prio clients and submit large amounts of
   false input in order to spoil aggregations.  Deployments could
   require clients to authenticate before they may contribute inputs.
   For example, by requiring submissions to be signed with a key trusted
   by aggregators.  However some deployments may opt to accept the risk
   of false inputs to avoid having to figure out how to distribute
   trusted identities to clients.

8.1.2.2.  Client attestation

   In the current threat model, servers participating in the protocol
   have no insight into the activities of clients except that they have
   uploaded input into a Prio aggregation, meaning that clients could
   covertly leak a user's data into some other channel which compromises
   privacy.  If we introduce the notion of a trusted computing base
   which can attest to the properties or activities of a client, then
   users and aggregators can be assured that their private data only
   goes into Prio.  For instance, clients could use the trusted
   computing base to attest to software measurements over reproducible
   builds, or a trusted operating system could attest to the client's
   network activity, allowing external observers to be confident that no
   data is being exfiltrated.

8.1.2.3.  Trusted anonymizing and authenticating proxy

   While the input shares transmitted by clients to aggregators reveal
   nothing about the original input, the aggregator can still learn
   auxiliary information received messages (for instance, source IP or
   HTTP user agent), which can identify participating clients or permit
   some attacks on robustness.  This is worse if client authentication
   used, since incoming messages would be bound to a cryptographic
   identity.  Deployments could include a trusted anonymizing proxy,
   which would be responsible for receiving input shares from clients,
   stripping any identifying information from them (including client
   authentication) and forwarding them to aggregators.  There should
   still be a confidential and authenticated channel from the client to
   the aggregator to ensure that no actor besides the aggregator may
   decrypt the input shares.

8.1.2.4.  Multiple protocol runs

   Prio is _robust_ against malicious clients, and _private_ against
   malicious servers, but cannot provide robustness against malicious
   servers.  Any aggregator can simply emit bogus output shares and
   undetectably spoil aggregates.  If enough aggregators were available,
   this could be mitigated by running the protocol multiple times with
   distinct subsets of aggregators chosen so that no aggregator appears
   in all subsets and checking all the outputs against each other.  If
   all the protocol runs do not agree, then participants know that at
   least one aggregator is defective, and it may be possible to identify
   the defector (i.e., if a majority of runs agree, and a single
   aggregator appears in every run that disagrees).  See #22
   (https://github.com/abetterinternet/prio-documents/issues/22) for
   discussion.

8.1.3.  Security considerations

8.1.3.1.  Infrastructure diversity

   Prio deployments should ensure that aggregators do not have common
   dependencies that would enable a single vendor to reassemble inputs.
   For example, if all participating aggregators stored unencrypted
   input shares on the same cloud object storage service, then that
   cloud vendor would be able to reassemble all the input shares and
   defeat privacy.

8.2.  System requirements

8.2.1.  Data types

9.  IANA Considerations

   TODO

10.  References

10.1.  Normative References

   [FIPS180-4]
              Department of Commerce, National Institute of Standards
              and Technology, U.S., "NIST FIPS 180-4, Secure Hash
              Standard", March 2012,
              <http://csrc.nist.gov/publications/fips/fips180-4/fips-
              180-4.pdf>.

   [I-D.irtf-cfrg-hpke]
              Barnes, R. L., Bhargavan, K., Lipp, B., and C. A. Wood,
              "Hybrid Public Key Encryption", Work in Progress,
              Internet-Draft, draft-irtf-cfrg-hpke-10, 7 July 2021,
              <https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-
              hpke-10>.

10.2.  Informative References

   [BBCp19]   "Zero-Knowledge Proofs on Secret-Shared Data via Fully
              Linear PCPs", 5 January 2021,
              <https://eprint.iacr.org/2019/188>.

   [BBCp21]   "Lightweight Techniques for Private Heavy Hitters", 5
              January 2021, <https://eprint.iacr.org/2021/017>.

   [CB17]     Corrigan-Gibbs, H. and D. Boneh, "Prio: Private, Robust,
              and Scalable Computation of Aggregate Statistics", 14
              March 2017, <https://crypto.stanford.edu/prio/paper.pdf>.

Author's Address

   Some People
   Somewhere

   Email: over@therainbow.net
